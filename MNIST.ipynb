{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540d189a",
   "metadata": {},
   "source": [
    "The Goal of this Notebook is to understand the working of Normalizing flows by applying them on MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c46c50",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34d4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a8c58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if available else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "# (Adapted) Code from PyTorch's Resnet impl: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
    "# we are using resnet for s and t \n",
    "\n",
    "# Defining conv3*3 and conv1*1 functions \n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "# Basic Building Block\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            # norm_layer = nn.BatchNorm2d\n",
    "            norm_layer = nn.InstanceNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ab73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "    expansion: int = 1\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            #norm_layer = nn.BatchNorm2d\n",
    "            norm_layer = nn.InstanceNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60057fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2d(nn.modules.batchnorm._NormBase):\n",
    "    ''' Partially based on: \n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        https://discuss.pytorch.org/t/implementing-batchnorm-in-pytorch-problem-with-updating-self-running-mean-and-self-running-var/49314/5 \n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        eps=1e-5,\n",
    "        momentum=0.005,\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype, 'affine': False, 'track_running_stats': True}\n",
    "        super(BatchNorm2d, self).__init__(\n",
    "            num_features, eps, momentum, **factory_kwargs\n",
    "        )\n",
    "        \n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError(\"expected 4D input (got {}D input)\".format(input.dim()))\n",
    "\n",
    "    def forward(self, input, validation=False):\n",
    "        self._check_input_dim(input)\n",
    "    \n",
    "        if self.training:\n",
    "            unbiased_var, tmean = torch.var_mean(input, [0, 2, 3], unbiased=True)\n",
    "            mean = torch.mean(input, [0, 2, 3]) # along channel axis\n",
    "            unbiased_var = torch.var(input, [0, 2, 3], unbiased=True) # along channel axis\n",
    "            running_mean = (1.0 - self.momentum) * self.running_mean.detach() + self.momentum * mean\n",
    "            running_var = (1.0 - self.momentum) * self.running_var.detach() + self.momentum * unbiased_var\n",
    "            current_mean = running_mean.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            current_var = running_var.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            \n",
    "            denom = (current_var + self.eps)\n",
    "            y = (input - current_mean) / denom.sqrt()\n",
    "            \n",
    "            self.running_mean = running_mean\n",
    "            self.running_var = running_var\n",
    "            \n",
    "            return y, -0.5 * torch.log(denom)\n",
    "        else:\n",
    "            current_mean = self.running_mean.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            current_var = self.running_var.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            \n",
    "            if validation:\n",
    "                denom = (current_var + self.eps)\n",
    "                y = (input - current_mean) / denom.sqrt()\n",
    "            else:\n",
    "                # Reverse operation for testing\n",
    "                denom = (current_var + self.eps)\n",
    "                y = input * denom.sqrt() + current_mean\n",
    "                \n",
    "            return y, -0.5 * torch.log(denom)\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = tuple([-1] + list(shape))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.reshape(x, self.shape)\n",
    "\n",
    "def dense_backbone(shape, network_width):\n",
    "    input_width = shape[0] * shape[1] * shape[2]\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(input_width, network_width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(network_width, input_width),\n",
    "        Reshape(shape)\n",
    "    )\n",
    "\n",
    "def bottleneck_backbone(in_planes, planes):\n",
    "    return nn.Sequential(\n",
    "        conv3x3(in_planes, planes),\n",
    "        BasicBlock(planes, planes),\n",
    "        BasicBlock(planes, planes),\n",
    "        conv3x3(planes, in_planes),\n",
    "    )\n",
    "\n",
    "check_mask = {}\n",
    "check_mask_device = {}\n",
    "def checkerboard_mask(shape, to_device=True):\n",
    "    global check_mask, check_mask_device\n",
    "    if shape not in check_mask:\n",
    "        check_mask[shape] = 1 - np.indices(shape).sum(axis=0) % 2\n",
    "        check_mask[shape] = torch.Tensor(check_mask[shape])\n",
    "        \n",
    "    if to_device and shape not in check_mask_device:\n",
    "        check_mask_device[shape] = check_mask[shape].to(device)\n",
    "        \n",
    "    return check_mask_device[shape] if to_device else check_mask[shape]\n",
    "\n",
    "chan_mask = {}\n",
    "chan_mask_device = {}\n",
    "def channel_mask(shape, to_device=True):\n",
    "    assert len(shape) == 3, shape\n",
    "    assert shape[0] % 2 == 0, shape\n",
    "    global chan_mask, chan_mask_device\n",
    "    if shape not in chan_mask:\n",
    "        chan_mask[shape] = torch.cat([torch.zeros((shape[0] // 2, shape[1], shape[2])),\n",
    "                                      torch.ones((shape[0] // 2, shape[1], shape[2])),],\n",
    "                                      dim=0)\n",
    "        assert chan_mask[shape].shape == shape, (chan_mask[shape].shape, shape)\n",
    "        \n",
    "    if to_device and shape not in chan_mask_device:\n",
    "        chan_mask_device[shape] = chan_mask[shape].to(device)\n",
    "        \n",
    "    return chan_mask_device[shape] if to_device else chan_mask[shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd900d4",
   "metadata": {},
   "source": [
    "Main Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20f09ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingFlowMNist(nn.Module):\n",
    "    EPSILON = 1e-5\n",
    "    \n",
    "    def __init__(self, num_coupling=6, num_final_coupling=4, planes=64):\n",
    "        super(NormalizingFlowMNist, self).__init__()\n",
    "        self.num_coupling = num_coupling\n",
    "        self.num_final_coupling = num_final_coupling\n",
    "        self.shape = (1, 28, 28)\n",
    "        \n",
    "        self.planes = planes\n",
    "        self.s = nn.ModuleList()\n",
    "        self.t = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        \n",
    "        # Learnable scalar scaling parameters for outputs of S and T\n",
    "        self.s_scale = nn.ParameterList()\n",
    "        self.t_scale = nn.ParameterList()\n",
    "        self.t_bias = nn.ParameterList()\n",
    "        self.shapes = []\n",
    "      \n",
    "        shape = self.shape\n",
    "        for i in range(num_coupling):\n",
    "            self.s.append(bottleneck_backbone(shape[0], planes))\n",
    "            self.t.append(bottleneck_backbone(shape[0], planes))\n",
    "            \n",
    "            self.s_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            self.t_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            self.t_bias.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            \n",
    "            self.norms.append(BatchNorm2d(shape[0]))\n",
    "            \n",
    "            self.shapes.append(shape)\n",
    "           \n",
    "            if i % 6 == 2:\n",
    "                shape = (4 * shape[0], shape[1] // 2, shape[2] // 2)\n",
    "                \n",
    "            if i % 6 == 5:\n",
    "                # Factoring out half the channels\n",
    "                shape = (shape[0] // 2, shape[1], shape[2])\n",
    "                planes = 2 * planes\n",
    "       \n",
    "        # Final coupling layers checkerboard\n",
    "        for i in range(num_final_coupling):\n",
    "            self.s.append(bottleneck_backbone(shape[0], planes))\n",
    "            self.t.append(bottleneck_backbone(shape[0], planes))\n",
    "            \n",
    "            self.s_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            self.t_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            self.t_bias.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            \n",
    "            self.norms.append(BatchNorm2d(shape[0]))\n",
    "            \n",
    "            self.shapes.append(shape)\n",
    "           \n",
    "        self.validation = False\n",
    "    \n",
    "    def validate(self):\n",
    "        self.eval()\n",
    "        self.validation = True\n",
    "        \n",
    "    def train(self, mode=True):\n",
    "        nn.Module.train(self, mode)\n",
    "        self.validation = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training or self.validation:\n",
    "            s_vals = []\n",
    "            norm_vals = []\n",
    "            y_vals = []\n",
    "            \n",
    "            for i in range(self.num_coupling):\n",
    "                shape = self.shapes[i]\n",
    "                mask = checkerboard_mask(shape) if i % 6 < 3 else channel_mask(shape)\n",
    "                mask = mask if i % 2 == 0 else (1 - mask)\n",
    "               \n",
    "                t = (self.t_scale[i]) * self.t[i](mask * x) + (self.t_bias[i])\n",
    "                s = (self.s_scale[i]) * torch.tanh(self.s[i](mask * x))\n",
    "                y = mask * x + (1 - mask) * (x * torch.exp(s) + t)\n",
    "                s_vals.append(torch.flatten((1 - mask) * s))\n",
    "               \n",
    "                if self.norms[i] is not None:\n",
    "                    y, norm_loss = self.norms[i](y, validation=self.validation)\n",
    "                    norm_vals.append(norm_loss)\n",
    "                    \n",
    "                if i % 6 == 2:\n",
    "                    y = torch.nn.functional.pixel_unshuffle(y, 2)\n",
    "                    \n",
    "                if i % 6 == 5:\n",
    "                    factor_channels = y.shape[1] // 2\n",
    "                    y_vals.append(torch.flatten(y[:, factor_channels:, :, :], 1))\n",
    "                    y = y[:, :factor_channels, :, :]\n",
    "                    \n",
    "                x = y\n",
    "                \n",
    "            # Final checkboard coupling\n",
    "            for i in range(self.num_coupling, self.num_coupling + self.num_final_coupling):\n",
    "                shape = self.shapes[i]\n",
    "                mask = checkerboard_mask(shape)\n",
    "                mask = mask if i % 2 == 0 else (1 - mask)\n",
    "               \n",
    "                t = (self.t_scale[i]) * self.t[i](mask * x) + (self.t_bias[i])\n",
    "                s = (self.s_scale[i]) * torch.tanh(self.s[i](mask * x))\n",
    "                y = mask * x + (1 - mask) * (x * torch.exp(s) + t)\n",
    "                s_vals.append(torch.flatten((1 - mask) * s))\n",
    "                \n",
    "                if self.norms[i] is not None:\n",
    "                    y, norm_loss = self.norms[i](y, validation=self.validation)\n",
    "                    norm_vals.append(norm_loss)\n",
    "                \n",
    "                x = y\n",
    "\n",
    "            y_vals.append(torch.flatten(y, 1))\n",
    "            \n",
    "            # Return outputs and vars needed for determinant\n",
    "            return (torch.flatten(torch.cat(y_vals, 1), 1),\n",
    "                    torch.cat(s_vals), \n",
    "                    torch.cat([torch.flatten(v) for v in norm_vals]) if len(norm_vals) > 0 else torch.zeros(1),\n",
    "                    torch.cat([torch.flatten(s) for s in self.s_scale]))\n",
    "        else:\n",
    "            y = x\n",
    "            y_remaining = y\n",
    "           \n",
    "            layer_vars = np.prod(self.shapes[-1])\n",
    "            y = torch.reshape(y_remaining[:, -layer_vars:], (-1,) + self.shapes[-1])\n",
    "            y_remaining = y_remaining[:, :-layer_vars]\n",
    "            \n",
    "            # Reversed final checkboard coupling\n",
    "            for i in reversed(range(self.num_coupling, self.num_coupling + self.num_final_coupling)):\n",
    "                shape = self.shapes[i]\n",
    "                mask = checkerboard_mask(shape)\n",
    "                mask = mask if i % 2 == 0 else (1 - mask)\n",
    "                \n",
    "                if self.norms[i] is not None:\n",
    "                    y, _ = self.norms[i](y)\n",
    "              \n",
    "                t = (self.t_scale[i]) * self.t[i](mask * y) + (self.t_bias[i])\n",
    "                s = (self.s_scale[i]) * torch.tanh(self.s[i](mask * y))\n",
    "                x = mask * y + (1 - mask) * ((y - t) * torch.exp(-s))\n",
    "               \n",
    "                y = x           \n",
    "          \n",
    "            layer_vars = np.prod(shape)\n",
    "            y = torch.cat((y, torch.reshape(y_remaining[:, -layer_vars:], (-1,) + shape)), 1)\n",
    "            y_remaining = y_remaining[:, :-layer_vars]\n",
    "            \n",
    "            # Multi-scale coupling layers\n",
    "            for i in reversed(range(self.num_coupling)):\n",
    "                shape = self.shapes[i]\n",
    "                mask = checkerboard_mask(shape) if i % 6 < 3 else channel_mask(shape)\n",
    "                mask = mask if i % 2 == 0 else (1 - mask)\n",
    "              \n",
    "                if self.norms[i] is not None:\n",
    "                    y, _ = self.norms[i](y)\n",
    "                    \n",
    "                t = (self.t_scale[i]) * self.t[i](mask * y) + (self.t_bias[i])\n",
    "                s = (self.s_scale[i]) * torch.tanh(self.s[i](mask * y))\n",
    "                x = mask * y + (1 - mask) * ((y - t) * torch.exp(-s))\n",
    "               \n",
    "                if i % 6 == 3:\n",
    "                    x = torch.nn.functional.pixel_shuffle(x, 2)\n",
    "                    \n",
    "                y = x\n",
    "                \n",
    "                if i > 0 and i % 6 == 0:\n",
    "                    layer_vars = np.prod(shape)\n",
    "                    y = torch.cat((y, torch.reshape(y_remaining[:, -layer_vars:], (-1,) + shape)), 1)\n",
    "                    y_remaining = y_remaining[:, :-layer_vars]\n",
    "            \n",
    "            assert np.prod(y_remaining.shape) == 0\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d426b31",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ead6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = torch.tensor(np.pi).to(device)\n",
    "def loss_fn(y, s, norms, scale, batch_size):\n",
    "    # same as in 2D \n",
    "    logpx = -torch.sum(0.5 * torch.log(2 * PI) + 0.5 * y**2)\n",
    "    det = torch.sum(s)\n",
    "    norms = torch.sum(norms)\n",
    "    reg = 5e-5 * torch.sum(scale ** 2)\n",
    "    loss = -(logpx + det + norms) + reg\n",
    "    return torch.div(loss, batch_size), (-logpx, -det, -norms, reg)\n",
    "# Training\n",
    "def pre_process(x):\n",
    "    x = x * 255.\n",
    "    noise = torch.rand(x.shape, device=x.device)\n",
    "    x = x + noise\n",
    "    # Apply transform to deal with boundary effects (see realNVP paper)\n",
    "    x = torch.logit(0.05 + 0.90 * x / 256)\n",
    "    return x\n",
    "\n",
    "def post_process(x):\n",
    "    # Convert back to integer values\n",
    "    return torch.clip(x, min=0, max=1)\n",
    "\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                               ]))\n",
    "\n",
    "test_dataset = datasets.MNIST('data', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                              ]))\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size, report_iters=10, num_pixels=28*28):\n",
    "    size = len(dataloader)\n",
    "    prev = []\n",
    "    for batch, (X, _) in enumerate(dataloader):\n",
    "        # Transfer to GPU\n",
    "        X = pre_process(X)\n",
    "        X = X.to(device)\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        y, s, norms, scale = model(X)\n",
    "        loss, comps = loss_fn(y, s, norms, scale, batch_size)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        prev = [(name, x, x.grad) for name, x in model.named_parameters(recurse=True)]\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % report_iters == 0:\n",
    "            loss, current = loss.item(), batch\n",
    "            # Account for x/255 preprocessing\n",
    "            loss += num_pixels * np.log(255)\n",
    "            print(f\"loss: {loss:.2f} = -logpx[{comps[0]:.1f}], -det[{comps[1]:.1f}], -norms[{comps[2]:.1f}], reg[{comps[3]:.4f}]\"\n",
    "                  f\"; bits/pixel: {loss / num_pixels / np.log(2):>.2f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        \n",
    "def test_loop(dataloader, model, loss_fn, num_pixels=28*28):\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.validate()\n",
    "        for X, _ in dataloader:\n",
    "            X = pre_process(X)\n",
    "            X = X.to(device)\n",
    "            y, s, norms, scale = model(X)\n",
    "            loss, _ = loss_fn(y, s, norms, scale, batch_size)\n",
    "            test_loss += loss\n",
    "            \n",
    "        model.train()\n",
    "\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    # Account for x/255 preprocessing\n",
    "    test_loss += num_pixels * np.log(255)\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:.2f}; {test_loss / num_pixels / np.log(2):.2f} \\n\")\n",
    "    return test_loss\n",
    "\n",
    "learning_rate = 0.0005\n",
    "batch_size = 50\n",
    "epochs = 10 # increase for better results\n",
    "\n",
    "model = NormalizingFlowMNist(num_coupling=12, num_final_coupling=4, planes=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "best_validation = None\n",
    "PATH = 'checkpoints/' # Directory path\n",
    "\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True) \n",
    "\n",
    "print(\"\\n--- Starting Training ---\") \n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True) # Recreate loader for shuffle\n",
    "    print(f\"\\n--- Epoch {t+1}/{epochs} ---\") \n",
    "    train_loop(train_loader, model, loss_fn, optimizer, batch_size)\n",
    "    validation_loss = test_loop(test_loader, model, loss_fn)\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': validation_loss,\n",
    "    }, PATH + f'mnist-{t}.model')\n",
    "\n",
    "    if best_validation is None or validation_loss < best_validation:\n",
    "        best_validation = validation_loss\n",
    "        best_path = PATH + f'mnist-{t}.model' # Store the path of the best model\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Done - Best model saved at epoch corresponding to:\", best_path) # Print path "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d2e5c",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NormalizingFlowMNist(num_coupling=12, num_final_coupling=4, planes=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "checkpoint = torch.load(best_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "# DEBUG - Checkmodel[s]\n",
    "model.validate()\n",
    "with torch.no_grad():\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) #shuffle=True)\n",
    "    for x, _ in train_loader:\n",
    "        x_pre = pre_process(x).to(device)\n",
    "        y, s, norms, scale = model(x_pre)\n",
    "        print(y.shape)\n",
    "        break\n",
    "\n",
    "model.train()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    xp = model(y)\n",
    "    x_post = post_process(xp)\n",
    "\n",
    "diff = x.to(device) - x_post\n",
    "print(torch.any(torch.abs(diff) > 1 / 255))\n",
    "\n",
    "print(diff.shape)\n",
    "for i in range(batch_size):\n",
    "    if torch.any(torch.abs(diff[i]) > 1 / 255):\n",
    "        #print(diff[i])\n",
    "        for j in range(28):\n",
    "            for k in range(28):\n",
    "                if torch.any(torch.abs(diff[i, 0, j, k]) > 1 / 255):\n",
    "                    print(i, 1, j, k, diff[i, 0, j, k].cpu().numpy())\n",
    "                    break\n",
    "        break\n",
    "\n",
    "s = pd.Series(torch.flatten(y).cpu().numpy())\n",
    "print(s.describe())\n",
    "s.hist(bins=50)\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(str(label))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "cols, rows = 8, 8\n",
    "with torch.no_grad():\n",
    "    X = torch.Tensor(torch.normal(torch.zeros(cols * rows, 28 * 28 * 1),\n",
    "                                  torch.ones(cols * rows, 28 * 28 * 1))).to(device)\n",
    "    Y = model(X)\n",
    "    samples = post_process(Y).cpu().numpy()\n",
    "\n",
    "figure = plt.figure(figsize=(15, 15))\n",
    "for i in range(1, cols * rows + 1):\n",
    "    img = samples[i - 1]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302782d",
   "metadata": {},
   "source": [
    "Visualizations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a0cf9",
   "metadata": {},
   "source": [
    "1) Generating samples resembling a specific digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a58f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "\n",
    "\n",
    "def get_latents_for_digit(model, dataset, digit, num_samples, batch_size, device):\n",
    "    model.validate() \n",
    "    latents = []\n",
    "    indices = [i for i, (_, label) in enumerate(dataset) if label == digit]\n",
    "    if len(indices) < num_samples:\n",
    "        print(f\"Warning: Only found {len(indices)} samples for digit {digit}, requested {num_samples}\")\n",
    "        num_samples = len(indices)\n",
    "\n",
    "    # Get a random subset of indices for the target digit\n",
    "    random_indices = random.sample(indices, num_samples)\n",
    "    subset = Subset(dataset, random_indices)\n",
    "    loader = DataLoader(subset, batch_size=batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, _ in loader:\n",
    "            X_pre = pre_process(X).to(device)\n",
    "            # In validate mode, forward pass maps image -> latent\n",
    "            y, _, _, _ = model(X_pre)\n",
    "            latents.append(y.cpu()) #\n",
    "\n",
    "    model.train(False) \n",
    "    model.eval()\n",
    "    if not latents:\n",
    "         raise ValueError(f\"No samples found for digit {digit}\")\n",
    "    return torch.cat(latents, dim=0)\n",
    "\n",
    "def generate_digit_samples(model, dataset, digit, num_to_generate, num_ref_samples, noise_std, batch_size, device):\n",
    "    print(f\"\\n--- Generating {num_to_generate} samples for digit {digit} ---\")\n",
    "    ref_latents = get_latents_for_digit(model, dataset, digit, num_ref_samples, batch_size, device)\n",
    "\n",
    "    z_mean = ref_latents.mean(dim=0, keepdim=True)\n",
    "    print(f\"Calculated mean latent vector shape: {z_mean.shape}\")\n",
    "\n",
    "    z_samples = z_mean.repeat(num_to_generate, 1)\n",
    "    noise = torch.randn_like(z_samples) * noise_std\n",
    "    z_samples = (z_samples + noise).to(device)\n",
    "    print(f\"Generated noisy latent samples shape: {z_samples.shape}\")\n",
    "    model.eval() # Ensure inverse pass mode\n",
    "    generated_images_pre = []\n",
    "    gen_batch_size = min(batch_size, num_to_generate)\n",
    "    with torch.no_grad():\n",
    "         for i in range(0, num_to_generate, gen_batch_size):\n",
    "             z_batch = z_samples[i:i+gen_batch_size]\n",
    "             # In eval mode, forward pass maps latent -> image\n",
    "             y_gen = model(z_batch)\n",
    "             generated_images_pre.append(y_gen.cpu())\n",
    "\n",
    "    generated_images_pre = torch.cat(generated_images_pre, dim=0)\n",
    "    print(f\"Generated images (before post-processing) shape: {generated_images_pre.shape}\")\n",
    "    generated_images = post_process(generated_images_pre)\n",
    "    print(f\"Generated images (after post-processing) shape: {generated_images.shape}\")\n",
    "\n",
    "    cols = 8\n",
    "    rows = (num_to_generate + cols - 1) // cols\n",
    "    figure = plt.figure(figsize=(cols * 1.5, rows * 1.5))\n",
    "    plt.suptitle(f\"Generated Samples for Digit: {digit}\", fontsize=16)\n",
    "    for i in range(num_to_generate):\n",
    "        img = generated_images[i]\n",
    "        figure.add_subplot(rows, cols, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "    plt.show()\n",
    "\n",
    "# --- Parameters for Generation ---\n",
    "DIGIT_TO_GENERATE = 3   \n",
    "NUM_SAMPLES_TO_GENERATE = 32 # How many samples to show\n",
    "NUM_REFERENCE_SAMPLES = 100 # How many real images to use to find the average latent space\n",
    "NOISE_STANDARD_DEVIATION = 0.6 # How much noise to add to the average latent vector (controls variety)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(best_path) # Or specify the path directly\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "generate_digit_samples(\n",
    "    model=model,\n",
    "    dataset=train_dataset,\n",
    "    digit=DIGIT_TO_GENERATE,\n",
    "    num_to_generate=NUM_SAMPLES_TO_GENERATE,\n",
    "    num_ref_samples=NUM_REFERENCE_SAMPLES,\n",
    "    noise_std=NOISE_STANDARD_DEVIATION,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287c1a2",
   "metadata": {},
   "source": [
    "2) Interpolate between two specific images and save as a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde443a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio # For creating Videos\n",
    "from IPython.display import Video as IPVideo, display \n",
    "import random \n",
    "def interpolate_images_to_video(model, dataset, source_idx, target_idx, num_steps, filename, fps, device, batch_size_override=None):\n",
    "    print(f\"\\n--- Interpolating from index {source_idx} to {target_idx} into video ---\")\n",
    "    assert filename.lower().endswith('.mp4'), \"Filename must end with .mp4\"\n",
    "\n",
    "    img_src, z_src, label_src = get_image_and_latent(model, dataset, source_idx, device) # FiX this \n",
    "    img_tgt, z_tgt, label_tgt = get_image_and_latent(model, dataset, target_idx, device)\n",
    "\n",
    "    print(f\"Source Label: {label_src}, Target Label: {label_tgt}\")\n",
    "    print(f\"Latent vector shape: {z_src.shape}\")\n",
    "\n",
    "    alphas = torch.linspace(0, 1, num_steps)\n",
    "    interpolated_latents = []\n",
    "    for alpha in alphas:\n",
    "        z_interp = (1 - alpha) * z_src + alpha * z_tgt\n",
    "        interpolated_latents.append(z_interp)\n",
    "\n",
    "    interpolated_latents = torch.cat(interpolated_latents, dim=0).to(device)\n",
    "    print(f\"Interpolated latents batch shape: {interpolated_latents.shape}\")\n",
    "\n",
    "    model.eval() # Ensure inverse pass mode\n",
    "    generated_images_pre = []\n",
    "    gen_batch_size = batch_size_override if batch_size_override else min(128, num_steps) # Use override or default\n",
    "    print(f\"Using generation batch size: {gen_batch_size}\")\n",
    "    with torch.no_grad():\n",
    "         for i in range(0, num_steps, gen_batch_size):\n",
    "             z_batch = interpolated_latents[i:i+gen_batch_size]\n",
    "             # In eval mode, forward pass maps latent -> image\n",
    "             y_gen_output = model(z_batch)\n",
    "             y_gen_tensor = y_gen_output[0] if isinstance(y_gen_output, tuple) else y_gen_output\n",
    "             generated_images_pre.append(y_gen_tensor.cpu())\n",
    "\n",
    "    generated_images_pre = torch.cat(generated_images_pre, dim=0)\n",
    "    print(f\"Generated interpolated images (before post-processing) shape: {generated_images_pre.shape}\")\n",
    "    interpolated_images = post_process(generated_images_pre)\n",
    "    print(f\"Generated interpolated images (after post-processing) shape: {interpolated_images.shape}\")\n",
    "\n",
    "    # --- Create Video Frames ---\n",
    "    frames = []\n",
    "    print(\"Preparing video frames...\")\n",
    "    for i in range(num_steps):\n",
    "        img_tensor = interpolated_images[i].squeeze() # Remove channel dim (H, W)\n",
    "        # Convert grayscale (H, W) float [0,1] to RGB uint8 [0,255] (H, W, 3)\n",
    "        # Most video codecs prefer RGB. Stack the grayscale channel 3 times.\n",
    "        frame_np = np.stack([img_tensor.numpy()] * 3, axis=-1)\n",
    "        frame_np_uint8 = (frame_np * 255).astype(np.uint8)\n",
    "        frames.append(frame_np_uint8)\n",
    "    print(f\"Prepared {len(frames)} frames.\")\n",
    "\n",
    "    # --- Save as MP4 Video ---\n",
    "    print(f\"Saving MP4 video to {filename}...\")\n",
    "    try:\n",
    "        imageio.mimwrite(filename, frames, fps=fps, codec='libx264', quality=8, macro_block_size=1)\n",
    "        print(\"MP4 video saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n-------------------------------------\")\n",
    "        print(f\"Error saving MP4 video: {e}\")\n",
    "        print(\"Ensure imageio and ffmpeg backend are installed correctly.\")\n",
    "        print(\"Try: pip install imageio-ffmpeg\")\n",
    "        print(\"-------------------------------------\")\n",
    "        return # Exit if saving failed\n",
    "    print(\"\\nAttempting to display generated video:\")\n",
    "    try:\n",
    "        display(IPVideo(filename, embed=True, width=200)) # Smaller display width\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Video file '{filename}' not found. Could not display.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display video inline: {e}\")\n",
    "\n",
    "\n",
    "# --- Parameters for Interpolation Video ---\n",
    "SOURCE_IMAGE_INDEX = 10   # Index from the dataset \n",
    "TARGET_IMAGE_INDEX = 20   # Index from the dataset\n",
    "INTERPOLATION_STEPS = 60 # Number of frames in the video \n",
    "VIDEO_FPS = 15           # Frames per second for the video\n",
    "VIDEO_FILENAME = f'mnist_interpolation_{SOURCE_IMAGE_INDEX}_to_{TARGET_IMAGE_INDEX}.mp4'\n",
    "GENERATION_BATCH_SIZE = 64 \n",
    "\n",
    "checkpoint = torch.load(best_path, map_location=device) # Load to correct device\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "interpolate_images_to_video(\n",
    "    model=model,\n",
    "    dataset=test_dataset, # Use train or test dataset\n",
    "    source_idx=SOURCE_IMAGE_INDEX,\n",
    "    target_idx=TARGET_IMAGE_INDEX,\n",
    "    num_steps=INTERPOLATION_STEPS,\n",
    "    filename=VIDEO_FILENAME,\n",
    "    fps=VIDEO_FPS,\n",
    "    device=device,\n",
    "    batch_size_override=GENERATION_BATCH_SIZE # Pass batch size for generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20536d4e",
   "metadata": {},
   "source": [
    "3) Generate an MP4 video of multiple interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d616f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "def generate_interpolation_grid_video(model, dataset, pairs, num_steps, grid_rows, filename, fps, device):\n",
    "    print(f\"\\n--- Generating interpolation video for {len(pairs)} pairs ---\")\n",
    "    num_pairs = len(pairs)\n",
    "    assert num_pairs == grid_rows * grid_rows, \"Number of pairs must match grid size (rows*rows)\"\n",
    "    assert filename.lower().endswith('.mp4'), \"Filename should end with .mp4 for video output\"\n",
    "\n",
    "    z_sources = []\n",
    "    z_targets = []\n",
    "\n",
    "    print(\"Getting latent vectors...\")\n",
    "    for source_idx, target_idx in pairs:\n",
    "        _, z_src, _ = get_image_and_latent(model, dataset, source_idx, device) # TODO: Fix this\n",
    "        _, z_tgt, _ = get_image_and_latent(model, dataset, target_idx, device)\n",
    "        z_sources.append(z_src)\n",
    "        z_targets.append(z_tgt)\n",
    "\n",
    "    z_sources = torch.cat(z_sources, dim=0).to(device)\n",
    "    z_targets = torch.cat(z_targets, dim=0).to(device)\n",
    "    print(f\"Source latents batch shape: {z_sources.shape}\")\n",
    "    print(f\"Target latents batch shape: {z_targets.shape}\")\n",
    "\n",
    "    model.eval() # Ensure inverse pass mode\n",
    "    frames = []\n",
    "    alphas = torch.linspace(0, 1, num_steps)\n",
    "    print(\"Generating video frames...\")\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        z_interp_batch = (1 - alpha) * z_sources + alpha * z_targets\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_images_pre = model(z_interp_batch) # Latent -> Image\n",
    "\n",
    "        # Post-process\n",
    "        interpolated_images = post_process(generated_images_pre).cpu()\n",
    "        grid_img = vutils.make_grid(interpolated_images, nrow=grid_rows, padding=2, normalize=False)\n",
    "        frame = grid_img.permute(1, 2, 0).numpy()\n",
    "        frame = (frame * 255).astype(np.uint8)\n",
    "        frames.append(frame)\n",
    "        print(f\"  Generated frame {i+1}/{num_steps} for alpha = {alpha:.2f}\", end='\\r')\n",
    "\n",
    "    print(f\"Saving MP4 video to {filename}...\")\n",
    "    try:\n",
    "        imageio.mimwrite(filename, frames, fps=fps, codec='libx264', quality=8, macro_block_size=1)\n",
    "        print(\"MP4 video saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n-------------------------------------\")\n",
    "        print(f\"Error saving MP4 video: {e}\")\n",
    "        print(\"This often means 'ffmpeg' is not installed or not found by imageio.\")\n",
    "        print(\"Try installing it:\")\n",
    "        print(\"  Using pip:   pip install imageio-ffmpeg\")\n",
    "        print(\"  Using conda: conda install ffmpeg -c conda-forge\")\n",
    "        print(\"  Or install ffmpeg via your system's package manager (apt, brew, etc.)\")\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "\n",
    "# --- Parameters for Grid Video ---\n",
    "NUM_GRID_ROWS = 4 # Creates a 4x4 grid\n",
    "NUM_PAIRS = NUM_GRID_ROWS * NUM_GRID_ROWS\n",
    "GRID_INTERPOLATION_STEPS = 120 # Number of frames for smoother video\n",
    "VIDEO_FPS = 15 # Frames per second for the output video\n",
    "\n",
    "# You could also manually select specific indices for interesting transitions\n",
    "image_pairs = []\n",
    "available_indices = list(range(len(test_dataset))) \n",
    "random.shuffle(available_indices)\n",
    "for _ in range(NUM_PAIRS):\n",
    "     if len(available_indices) < 2:\n",
    "         print(\"Warning: Not enough unique indices left in dataset for pairs.\")\n",
    "         break\n",
    "     idx1 = available_indices.pop()\n",
    "     idx2 = available_indices.pop()\n",
    "     image_pairs.append((idx1, idx2))\n",
    "\n",
    "print(\"Selected pairs (source_idx, target_idx):\", image_pairs)\n",
    "\n",
    "# best_path should be defined from your training loop\n",
    "checkpoint = torch.load(best_path) \n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "video_filename = 'mnist_interpolation_grid.mp4'\n",
    "generate_interpolation_grid_video( \n",
    "    model=model,\n",
    "    dataset=test_dataset, # Use train or test dataset\n",
    "    pairs=image_pairs,\n",
    "    num_steps=GRID_INTERPOLATION_STEPS,\n",
    "    grid_rows=NUM_GRID_ROWS,\n",
    "    filename=video_filename, \n",
    "    fps=VIDEO_FPS,         \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Optional: Display the generated Video inline (if in Jupyter and file saved)\n",
    "print(\"\\nAttempting to display generated video (may not work in all environments):\")\n",
    "try:\n",
    "    display(IPVideo(video_filename, embed=True, width=400))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Video file '{video_filename}' not found. Could not display.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not display video inline: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b19b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Fix some functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
